{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型融合\n",
    "\n",
    "模型融合，先产生一组学习器，再通过某种策略将他们结合起来，以加强模型效果，常见的模型融合方法包括voting，soft voting，averaging，ranking，blending和stacking。\n",
    "\n",
    "### 手写stacking\n",
    "\n",
    "\n",
    "使用stacking融合模型，第一层使用lightgbm、xgboost、catboost，第二层使用较简单的模型，如逻辑回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy import sparse\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import xgboost\n",
    "import lightgbm\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "def stacking(clf, train_x, train_y, test_x, clf_name, kf, label_split=None):\n",
    "    train = np.zeros((train_x.shape[0], 1))\n",
    "    test = np.zeros((test_x.shape[0], 1))\n",
    "    test_pre = np.empty((folds, test_x.shape[0], 1))\n",
    "    cv_scores = []\n",
    "    for i, (train_index, val_index) in enumerate(kf.split(train_x, train_y)):\n",
    "        tr_x = train_x.iloc[train_index]\n",
    "        tr_y = train_y[train_index]\n",
    "        val_x = train_x.iloc[val_index]\n",
    "        val_y = train_y[val_index]\n",
    "        if clf_name in ['rf', 'ada', 'gb', 'et', 'lr']:\n",
    "            print('clf_name:', clf_name)\n",
    "            clf.fit(tr_x, tr_y)\n",
    "            val_pred = clf.predict(val_x).reshape(-1,1)\n",
    "            test_pre[i,:] = clf.predict(test_x).reshape(-1,1)\n",
    "            cv_scores.append(roc_auc_score(val_y, val_pred))\n",
    "            print('{}折 cv_scores:{}'.format(i, cv_scores))\n",
    "            \n",
    "        elif clf_name == \"lgb\":\n",
    "            train_matrix = clf.Dataset(tr_x, label=tr_y)\n",
    "            valid_matrix = clf.Dataset(val_x, label=val_y)\n",
    "            params = {\n",
    "                'boosting_type': 'gbdt',\n",
    "                'objective': 'binary',\n",
    "                'max_bin':200,\n",
    "                'metric': 'auc',\n",
    "                'min_child_weight': 1.5,\n",
    "                'num_leaves': 2 ** 4,\n",
    "                'lambda_l2': 10,\n",
    "                'learning_rate': 0.03,\n",
    "                'feature_fraction': 0.8,\n",
    "                'bagging_fraction': 0.8,\n",
    "                'bagging_freq': 20,\n",
    "                'seed': 2020,\n",
    "                'nthread': -1,\n",
    "                'verbose': -1,\n",
    "            }\n",
    "            model = clf.train(params, train_matrix, 10000, valid_sets=[train_matrix, valid_matrix], verbose_eval=200,early_stopping_rounds=200)\n",
    "            \n",
    "            val_pred = model.predict(val_x, num_iteration=model.best_iteration).reshape(-1,1)\n",
    "            train[val_index] = val_pred\n",
    "            test_pre[i, :] = model.predict(test_x, num_iteration=model.best_iteration).reshape(-1,1)\n",
    "            \n",
    "            cv_scores.append(roc_auc_score(val_y, val_pred))\n",
    "            print('{}折 cv_scores:{}'.format(i, cv_scores))\n",
    "            \n",
    "        elif clf_name == \"xgb\":\n",
    "            train_matrix = clf.DMatrix(tr_x , label=tr_y)\n",
    "            valid_matrix = clf.DMatrix(val_x , label=val_y)\n",
    "            params = {\n",
    "                'booster': 'gbtree',\n",
    "                'objective': 'binary:logistic',\n",
    "                'eval_metric': 'auc',\n",
    "                'gamma': 1,\n",
    "                'min_child_weight': 2,\n",
    "                'max_depth': 5,\n",
    "                'lambda': 10,\n",
    "                'subsample': 0.85,\n",
    "                'colsample_bytree': 0.7,\n",
    "                'colsample_bylevel': 1,\n",
    "                'eta': 0.03,\n",
    "                'tree_method': 'exact',\n",
    "                'seed': 2019,\n",
    "                'nthread': 12,\n",
    "                \"silent\": True,\n",
    "                'scale_pos_weight':3 , \n",
    "            }\n",
    "            watchlist = [(train_matrix, 'train'),(valid_matrix, 'eval')]\n",
    "            model = clf.train(params, train_matrix, num_boost_round=50000, evals=watchlist, verbose_eval=200, early_stopping_rounds=200)\n",
    "            \n",
    "            val_pred = model.predict(valid_matrix, ntree_limit=model.best_ntree_limit).reshape(-1,1)\n",
    "            train[val_index] = val_pred\n",
    "            \n",
    "            test_pre[i, :] = model.predict(clf.DMatrix(test_x), ntree_limit=model.best_ntree_limit).reshape(-1,1)\n",
    "            \n",
    "            cv_scores.append(roc_auc_score(val_y, val_pred))\n",
    "            print('{}折 cv_scores:{}'.format(i, cv_scores))\n",
    "            \n",
    "        elif clf_name == \"cat\":\n",
    "            params = {\n",
    "                'learning_rate': 0.03,\n",
    "                'depth': 5,\n",
    "                'l2_leaf_reg': 10,\n",
    "                'bootstrap_type': 'Bernoulli',\n",
    "                'od_type': 'Iter',\n",
    "                'od_wait': 50,\n",
    "                'random_seed': 2018,\n",
    "                'subsample':0.8,\n",
    "                'colsample_bylevel': 0.85,\n",
    "                'allow_writing_files': False}\n",
    "            model = clf(iterations=20000, **params)\n",
    "            model.fit(tr_x, tr_y, eval_set=(val_x, val_y), cat_features=[], use_best_model=True, verbose=500)\n",
    "            val_pred = model.predict(val_x).reshape(-1,1)\n",
    "            train[val_index] = val_pred\n",
    "            test_pre[i, :] = model.predict(test_x).reshape(-1,1)\n",
    "            cv_scores.append(roc_auc_score(val_y, val_pred))\n",
    "            print('{}折 cv_scores:{}'.format(i, cv_scores))\n",
    "        test[:] = test_pre.mean(axis=0)\n",
    "    print(\"%s_score_list:\" % clf_name, cv_scores)\n",
    "    print(\"%s_score_mean:\" % clf_name, np.mean(cv_scores))\n",
    "    print(\"%s_score_std:\" % clf_name, np.std(cv_scores))\n",
    "    return train.reshape(-1,1), test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_model(train_x, train_y, test_x, kf, label_split=None):\n",
    "    rf = RandomForestClassifier(n_estimators=200, max_depth=20, n_jobs=-1, random_state=2020, max_features=\"auto\", verbose=1)\n",
    "    rf_train, rf_test = stacking(rf, train_x, train_y, test_x, \"rf\", kf, label_split)\n",
    "    return rf_train, rf_test, \"rf_stacker\"\n",
    "\n",
    "def ada_model(train_x, train_y, test_x, kf, label_split=None):\n",
    "    adaboost = AdaBoostClassifier(n_estimators=200, random_state=2020,learning_rate=0.01, n_jobs=-1)\n",
    "    ada_train, ada_test = stacking(adaboost, train_x, train_y, test_x, \"ada\", kf, label_split)\n",
    "    return ada_train, ada_test, \"ada_stacker\"\n",
    "\n",
    "\n",
    "def gb_model(train_x, train_y, test_x, kf, label_split=None):\n",
    "    gbdt = GradientBoostingClassifier(learning_rate=0.01, n_estimators=200, subsample=0.8, random_state=2020,max_depth=5,verbose=1)\n",
    "    gbdt_train, gbdt_test = stacking(gbdt, train_x, train_y, test_x, \"gbdt\", kf, label_split)\n",
    "    return gbdt_train, gbdt_test, \"gbdt_stacker\"\n",
    "\n",
    "\n",
    "def et_model(train_x, train_y, test_x, kf, label_split=None):\n",
    "    et = ExtraTreesClassifier(n_estimators=200, random_state=2020,learning_rate=0.01, max_depth=15,max_features=\"auto\",verbose=1)\n",
    "    et_train, et_test = stacking(et, train_x, train_y, test_x, \"et\", kf, label_split)\n",
    "    return et_train, et_test, \"et_stacker\"\n",
    "\n",
    "\n",
    "def lr_model(train_x, train_y, test_x, kf, label_split=None):\n",
    "    lr = LogisticRegression(n_jobs=-1)\n",
    "    lr_train, lr_test = stacking(lr, train_x, train_y, test_x, \"lr\", kf, label_split)\n",
    "    return lr_train, lr_test, \"lr_stacker\"\n",
    "\n",
    "def my_xgb_model(train_x, train_y, test_x, kf, label_split=None):\n",
    "    xgb_train, xgb_test = stacking(xgboost, train_x, train_y, test_x, \"xgb\", kf, label_split)\n",
    "    return xgb_train, xgb_test, \"xgb_stacker\"\n",
    "\n",
    "def my_lgb_model(train_x, train_y, test_x, kf, label_split=None):\n",
    "    lgb_train, lgb_test = stacking(lightgbm, train_x, train_y, test_x, \"lgb\", kf, label_split)\n",
    "    return lgb_train, lgb_test, \"lgb_stacker\"\n",
    "\n",
    "def my_cat_model(train_x, train_y, test_x, kf, label_split=None):\n",
    "    cat_train, cat_test = stacking(CatBoostRegressor, train_x, train_y, test_x, \"cat\", kf, label_split)\n",
    "    return cat_train, cat_test, \"cat_stacker\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacking_pred(train_x, train_y, test_x, kf, clf_list, label_split=None, clf_fin=\"lgb\", if_concat_origin=True):\n",
    "    print(datetime.datetime.now())\n",
    "    column_list = []\n",
    "    train_data_list = []\n",
    "    test_data_list = []\n",
    "    for i, clf_list in enumerate(clf_list):\n",
    "        clf_list = [clf_list]\n",
    "\n",
    "        for clf in clf_list:\n",
    "            train_data, test_data, clf_name = clf(train_x, train_y, test_x, kf, label_split)\n",
    "            train_data_list.append(train_data)\n",
    "            test_data_list.append(test_data)\n",
    "            column_list.append(\"clf_%s\"%(clf_name))\n",
    "    train = np.concatenate(train_data_list, axis=1)\n",
    "    test = np.concatenate(test_data_list, axis=1)\n",
    "    if if_concat_origin:\n",
    "        train = np.concatenate([train_x, train], axis=1)\n",
    "        test = np.concatenate([test_x, test], axis=1)\n",
    "    print(train_x.shape)\n",
    "    print(train.shape)\n",
    "    print(column_list)\n",
    "    if clf_fin == \"xgb\":\n",
    "        print('second layer model:', clf_fin)\n",
    "        clf = xgboost\n",
    "        train_matrix = clf.DMatrix(train , label=train_y)\n",
    "        valid_matrix = clf.DMatrix(train , label=train_y)\n",
    "        params = {\n",
    "            'booster': 'gbtree',\n",
    "            'objective': 'binary:logistic',\n",
    "            'eval_metric': 'auc',\n",
    "            'gamma': 1,\n",
    "            'min_child_weight': 1.5,\n",
    "            'max_depth': 5,\n",
    "            'lambda': 10,\n",
    "            'subsample': 0.7,\n",
    "            'colsample_bytree': 0.7,\n",
    "            'colsample_bylevel': 0.7,\n",
    "            'eta': 0.04,\n",
    "            'tree_method': 'exact',\n",
    "            'seed': 2019,\n",
    "            \"silent\": True,\n",
    "        }\n",
    "        watchlist = [(train_matrix, 'train'),(valid_matrix, 'eval')]\n",
    "        model = clf.train(params, train_matrix, num_boost_round=5000, evals=watchlist, verbose_eval=200, early_stopping_rounds=200)\n",
    "\n",
    "        pre = model.predict(clf.DMatrix(test), ntree_limit=model.best_ntree_limit).reshape(-1,1)\n",
    "        return pre, train, test\n",
    "    if clf_fin == 'lgb':\n",
    "        print('second layer model:', clf_fin)\n",
    "        clf = lightgbm\n",
    "        train_matrix = clf.Dataset(train, label=train_y)\n",
    "        valid_matrix = clf.Dataset(train, label=train_y)\n",
    "        params = {\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': 'binary',\n",
    "            'metric': 'auc',\n",
    "            'min_child_weight': 5,\n",
    "            'num_leaves': 2 ** 5,\n",
    "            'lambda_l2': 10,\n",
    "            'feature_fraction': 0.8,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'bagging_freq': 4,\n",
    "            'learning_rate': 0.1,\n",
    "            'seed': 2018,\n",
    "            'silent': True,\n",
    "            'verbose': -1,\n",
    "        }\n",
    "        model = clf.train(params, train_matrix, 5000, valid_sets=[train_matrix, valid_matrix], verbose_eval=200,early_stopping_rounds=200)\n",
    "        pre = model.predict(test, num_iteration=model.best_iteration).reshape(-1,1)\n",
    "        return pre, train, test\n",
    "    if clf_fin == 'lr':\n",
    "        print('second layer model:', clf_fin)\n",
    "        clf = LogisticRegression(n_jobs=-1)\n",
    "        clf.fit(train, train_y)\n",
    "        pre = clf.predict_proba(test)\n",
    "        return pre, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "folds = 5\n",
    "seed = 2020\n",
    "kf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=0)\n",
    "\n",
    "clf_list = [my_lgb_model, my_xgb_model, my_cat_model ]\n",
    "#clf_list = [rf_model  ]\n",
    "\n",
    "pre, train, test = stacking_pred(train_data, target, test_data, kf, clf_list, label_split=None, clf_fin='lr', if_concat_origin=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python36]",
   "language": "python",
   "name": "conda-env-python36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
