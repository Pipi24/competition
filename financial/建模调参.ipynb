{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建模调参\n",
    "\n",
    "建模调参首先是要建立模型，然后调整参数，使得模型的性能在已有数据特征上达到最优，根据具体任务的不同，大体可以分为分类和回归两类模型，比赛中常用的机器模型包括逻辑回归、决策树、随机森林、lightgbm、xgboost等，在特征工程阶段要结合使用的模型对数据进行合理的处理，才能使模型发挥最大的性能。在建模后，需要对模型进行评估，常用的评估指标包括准确率、F值、AUC曲线、MSE等。通过分析模型学习过程以及结果，我们应对模型的参数进行调整，如过拟合时要调整对应参数来降低过拟合。\n",
    "\n",
    "### 模型建立\n",
    "\n",
    "1. 集成树模型\n",
    "\n",
    "使用lightgbm、xgboost、catboost进行建模，每一个模型的原理和使用方法不赘述，参见：\n",
    "\n",
    "lightgbm：https://lightgbm.apachecn.org/#/\n",
    "\n",
    "xgboost：https://xgboost.apachecn.org/#/\n",
    "\n",
    "catboost：https://catboost.ai/docs/\n",
    "\n",
    "使用交叉验证验证模型性能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "def cv_model(clf, train_x, train_y, test_x, clf_name):\n",
    "    print(datetime.datetime.now())\n",
    "    folds = 5\n",
    "    seed = 2020\n",
    "    kf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "    train = np.zeros(train_x.shape[0])\n",
    "    test = np.zeros(test_x.shape[0])\n",
    "    cv_scores = []\n",
    "    for i, (train_index, valid_index) in enumerate(kf.split(train_x, train_y)):\n",
    "        print('************************************ {} ************************************'.format(str(i+1)))\n",
    "        trn_x, trn_y, val_x, val_y = train_x.iloc[train_index], train_y[train_index], train_x.iloc[valid_index], train_y[valid_index]\n",
    "        if clf_name == \"lgb\":          \n",
    "            train_matrix = clf.Dataset(trn_x, label=trn_y, weight=trn_y.map(lambda x:1 if x==0 else 3))\n",
    "            #train_matrix = clf.Dataset(trn_x, label=trn_y)\n",
    "            valid_matrix = clf.Dataset(val_x, label=val_y)\n",
    "\n",
    "            params = {\n",
    "                'boosting_type': 'gbdt',\n",
    "                'objective': 'binary',\n",
    "                'max_bin':200,\n",
    "                'metric': 'auc',\n",
    "                'min_child_weight': 1.5,\n",
    "                'num_leaves': 2 ** 4,\n",
    "                'lambda_l2': 10,\n",
    "                'learning_rate': 0.1,\n",
    "                'feature_fraction': 0.8,\n",
    "                'bagging_fraction': 0.8,\n",
    "                'bagging_freq': 20,\n",
    "                'seed': 2020,\n",
    "                'nthread': -1,\n",
    "                'verbose': -1,\n",
    "            }\n",
    "            params = {\n",
    "                'objective' : 'binary',\n",
    "                'is_unbalance' : True,\n",
    "                'metric' : 'auc',\n",
    "                'max_depth' : 9,\n",
    "                'num_leaves' : 75,\n",
    "                'learning_rate' : 0.1,\n",
    "                'min_child_samples' : 40,\n",
    "                'min_child_weight' : 1,\n",
    "                'colsample_bytree' : 0.7,\n",
    "                'subsample' : 0.9,\n",
    "                'subsample_freq' : 4,\n",
    "                'reg_alpha' : 0.4,\n",
    "                'reg_lambda' : 35,\n",
    "                'cat_smooth' : 0,\n",
    "                'seed': 2020,\n",
    "                'nthread': -1,\n",
    "                'verbose': -1,\n",
    "            }\n",
    "\n",
    "            model = clf.train(params, train_matrix, 10000, valid_sets=[train_matrix, valid_matrix], verbose_eval=200,early_stopping_rounds=200)\n",
    "            val_pred = model.predict(val_x, num_iteration=model.best_iteration)\n",
    "            test_pred = model.predict(test_x, num_iteration=model.best_iteration)\n",
    "            #print(\"importance: \", model.feature_importances_attribute)\n",
    "            #lightgbm.plot_importance(model)\n",
    "            im=pd.DataFrame({'importance':model.feature_importance(),'var':trn_x.columns})\n",
    "            im=im.sort_values(by='importance',ascending=False)\n",
    "            print(im)\n",
    "        if clf_name == \"xgb\":\n",
    "            train_matrix = clf.DMatrix(trn_x , label=trn_y)\n",
    "            valid_matrix = clf.DMatrix(val_x , label=val_y)\n",
    "            params = {\n",
    "                'booster': 'gbtree',\n",
    "                'objective': 'binary:logistic',\n",
    "                'eval_metric': 'auc',\n",
    "                'gamma': 1,\n",
    "                'min_child_weight': 2,\n",
    "                'max_depth': 5,\n",
    "                'lambda': 10,\n",
    "                'subsample': 0.85,\n",
    "                'colsample_bytree': 0.7,\n",
    "                'colsample_bylevel': 1,\n",
    "                'eta': 0.03,\n",
    "                'tree_method': 'exact',\n",
    "                'seed': 2019,\n",
    "                'nthread': 12,\n",
    "                \"silent\": True,\n",
    "                'scale_pos_weight':3 , \n",
    "            }\n",
    "            watchlist = [(train_matrix, 'train'),(valid_matrix, 'eval')]\n",
    "            model = clf.train(params, train_matrix, num_boost_round=10000, evals=watchlist, verbose_eval=200, early_stopping_rounds=100)\n",
    "            val_pred = model.predict(valid_matrix, ntree_limit=model.best_ntree_limit)\n",
    "            test_pred = model.predict(clf.DMatrix(test_x), ntree_limit=model.best_ntree_limit)\n",
    "            #print(\"importance: \", model.feature_importances_attribute)\n",
    "            \n",
    "            #clf.plot_importance()\n",
    "            display(pd.Series(model.get_fscore()).sort_values(ascending=False))\n",
    "            \n",
    "        if clf_name == \"cat\":\n",
    "            params = {\n",
    "                'learning_rate': 0.03,\n",
    "                'depth': 5,\n",
    "                'l2_leaf_reg': 10,\n",
    "                'bootstrap_type': 'Bernoulli',\n",
    "                'od_type': 'Iter',\n",
    "                'od_wait': 50,\n",
    "                'random_seed': 2018,\n",
    "                'subsample':0.8,\n",
    "                'colsample_bylevel': 0.85,\n",
    "                'allow_writing_files': False}\n",
    "            model = clf(iterations=20000, **params)\n",
    "            model.fit(trn_x, trn_y, eval_set=(val_x, val_y), cat_features=[], use_best_model=True, verbose=500)\n",
    "            val_pred = model.predict(val_x)\n",
    "            test_pred = model.predict(test_x)\n",
    "            print(\"importance: \", model.get_feature_importance(prettified=True))\n",
    "        train[valid_index] = val_pred\n",
    "        test += test_pred / kf.n_splits\n",
    "        cv_scores.append(roc_auc_score(val_y, val_pred))\n",
    "        print(cv_scores)\n",
    "    print(\"%s_scotrainre_list:\" % clf_name, cv_scores)\n",
    "    print(\"%s_score_mean:\" % clf_name, np.mean(cv_scores))\n",
    "    print(\"%s_score_std:\" % clf_name, np.std(cv_scores))\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "def lgb_model(x_train, y_train, x_test):\n",
    "    lgb_train, lgb_test = cv_model(lgb, x_train, y_train, x_test, \"lgb\")\n",
    "    return lgb_train, lgb_test\n",
    "def xgb_model(x_train, y_train, x_test):\n",
    "    xgb_train, xgb_test = cv_model(xgb, x_train, y_train, x_test, \"xgb\")\n",
    "    return xgb_train, xgb_test\n",
    "def cat_model(x_train, y_train, x_test):\n",
    "    cat_train, cat_test = cv_model(CatBoostRegressor, x_train, y_train, x_test, \"cat\")\n",
    "    return cat_train, cat_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_train, lgb_test = lgb_model(train_data, target, test_data)\n",
    "xgb_train, xgb_test = xgb_model(train_data, target, test_data)\n",
    "cat_train, cat_test = cat_model(train_data, target, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 模型融合\n",
    "\n",
    "使用stacking融合模型，第一层使用lightgbm、xgboost、catboost，第二层使用逻辑回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy import sparse\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import xgboost\n",
    "import lightgbm\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "def stacking(clf, train_x, train_y, test_x, clf_name, kf, label_split=None):\n",
    "    train = np.zeros((train_x.shape[0], 1))\n",
    "    test = np.zeros((test_x.shape[0], 1))\n",
    "    test_pre = np.empty((folds, test_x.shape[0], 1))\n",
    "    cv_scores = []\n",
    "    for i, (train_index, val_index) in enumerate(kf.split(train_x, train_y)):\n",
    "        tr_x = train_x.iloc[train_index]\n",
    "        tr_y = train_y[train_index]\n",
    "        val_x = train_x.iloc[val_index]\n",
    "        val_y = train_y[val_index]\n",
    "        if clf_name in ['rf', 'ada', 'gb', 'et', 'lr']:\n",
    "            print('clf_name:', clf_name)\n",
    "            clf.fit(tr_x, tr_y)\n",
    "            val_pred = clf.predict(val_x).reshape(-1,1)\n",
    "            test_pre[i,:] = clf.predict(test_x).reshape(-1,1)\n",
    "            cv_scores.append(roc_auc_score(val_y, val_pred))\n",
    "            print('{}折 cv_scores:{}'.format(i, cv_scores))\n",
    "            \n",
    "        elif clf_name == \"lgb\":\n",
    "            train_matrix = clf.Dataset(tr_x, label=tr_y)\n",
    "            valid_matrix = clf.Dataset(val_x, label=val_y)\n",
    "            params = {\n",
    "                'boosting_type': 'gbdt',\n",
    "                'objective': 'binary',\n",
    "                'max_bin':200,\n",
    "                'metric': 'auc',\n",
    "                'min_child_weight': 1.5,\n",
    "                'num_leaves': 2 ** 4,\n",
    "                'lambda_l2': 10,\n",
    "                'learning_rate': 0.03,\n",
    "                'feature_fraction': 0.8,\n",
    "                'bagging_fraction': 0.8,\n",
    "                'bagging_freq': 20,\n",
    "                'seed': 2020,\n",
    "                'nthread': -1,\n",
    "                'verbose': -1,\n",
    "            }\n",
    "            model = clf.train(params, train_matrix, 10000, valid_sets=[train_matrix, valid_matrix], verbose_eval=200,early_stopping_rounds=200)\n",
    "            \n",
    "            val_pred = model.predict(val_x, num_iteration=model.best_iteration).reshape(-1,1)\n",
    "            train[val_index] = val_pred\n",
    "            test_pre[i, :] = model.predict(test_x, num_iteration=model.best_iteration).reshape(-1,1)\n",
    "            \n",
    "            cv_scores.append(roc_auc_score(val_y, val_pred))\n",
    "            print('{}折 cv_scores:{}'.format(i, cv_scores))\n",
    "            \n",
    "        elif clf_name == \"xgb\":\n",
    "            train_matrix = clf.DMatrix(tr_x , label=tr_y)\n",
    "            valid_matrix = clf.DMatrix(val_x , label=val_y)\n",
    "            params = {\n",
    "                'booster': 'gbtree',\n",
    "                'objective': 'binary:logistic',\n",
    "                'eval_metric': 'auc',\n",
    "                'gamma': 1,\n",
    "                'min_child_weight': 2,\n",
    "                'max_depth': 5,\n",
    "                'lambda': 10,\n",
    "                'subsample': 0.85,\n",
    "                'colsample_bytree': 0.7,\n",
    "                'colsample_bylevel': 1,\n",
    "                'eta': 0.03,\n",
    "                'tree_method': 'exact',\n",
    "                'seed': 2019,\n",
    "                'nthread': 12,\n",
    "                \"silent\": True,\n",
    "                'scale_pos_weight':3 , \n",
    "            }\n",
    "            watchlist = [(train_matrix, 'train'),(valid_matrix, 'eval')]\n",
    "            model = clf.train(params, train_matrix, num_boost_round=50000, evals=watchlist, verbose_eval=200, early_stopping_rounds=200)\n",
    "            \n",
    "            val_pred = model.predict(valid_matrix, ntree_limit=model.best_ntree_limit).reshape(-1,1)\n",
    "            train[val_index] = val_pred\n",
    "            \n",
    "            test_pre[i, :] = model.predict(clf.DMatrix(test_x), ntree_limit=model.best_ntree_limit).reshape(-1,1)\n",
    "            \n",
    "            cv_scores.append(roc_auc_score(val_y, val_pred))\n",
    "            print('{}折 cv_scores:{}'.format(i, cv_scores))\n",
    "            \n",
    "        elif clf_name == \"cat\":\n",
    "            params = {\n",
    "                'learning_rate': 0.03,\n",
    "                'depth': 5,\n",
    "                'l2_leaf_reg': 10,\n",
    "                'bootstrap_type': 'Bernoulli',\n",
    "                'od_type': 'Iter',\n",
    "                'od_wait': 50,\n",
    "                'random_seed': 2018,\n",
    "                'subsample':0.8,\n",
    "                'colsample_bylevel': 0.85,\n",
    "                'allow_writing_files': False}\n",
    "            model = clf(iterations=20000, **params)\n",
    "            model.fit(tr_x, tr_y, eval_set=(val_x, val_y), cat_features=[], use_best_model=True, verbose=500)\n",
    "            val_pred = model.predict(val_x).reshape(-1,1)\n",
    "            train[val_index] = val_pred\n",
    "            test_pre[i, :] = model.predict(test_x).reshape(-1,1)\n",
    "            cv_scores.append(roc_auc_score(val_y, val_pred))\n",
    "            print('{}折 cv_scores:{}'.format(i, cv_scores))\n",
    "        test[:] = test_pre.mean(axis=0)\n",
    "    print(\"%s_score_list:\" % clf_name, cv_scores)\n",
    "    print(\"%s_score_mean:\" % clf_name, np.mean(cv_scores))\n",
    "    print(\"%s_score_std:\" % clf_name, np.std(cv_scores))\n",
    "    return train.reshape(-1,1), test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_model(train_x, train_y, test_x, kf, label_split=None):\n",
    "    rf = RandomForestClassifier(n_estimators=200, max_depth=20, n_jobs=-1, random_state=2020, max_features=\"auto\", verbose=1)\n",
    "    rf_train, rf_test = stacking(rf, train_x, train_y, test_x, \"rf\", kf, label_split)\n",
    "    return rf_train, rf_test, \"rf_stacker\"\n",
    "\n",
    "def ada_model(train_x, train_y, test_x, kf, label_split=None):\n",
    "    adaboost = AdaBoostClassifier(n_estimators=200, random_state=2020,learning_rate=0.01, n_jobs=-1)\n",
    "    ada_train, ada_test = stacking(adaboost, train_x, train_y, test_x, \"ada\", kf, label_split)\n",
    "    return ada_train, ada_test, \"ada_stacker\"\n",
    "\n",
    "\n",
    "def gb_model(train_x, train_y, test_x, kf, label_split=None):\n",
    "    gbdt = GradientBoostingClassifier(learning_rate=0.01, n_estimators=200, subsample=0.8, random_state=2020,max_depth=5,verbose=1)\n",
    "    gbdt_train, gbdt_test = stacking(gbdt, train_x, train_y, test_x, \"gbdt\", kf, label_split)\n",
    "    return gbdt_train, gbdt_test, \"gbdt_stacker\"\n",
    "\n",
    "\n",
    "def et_model(train_x, train_y, test_x, kf, label_split=None):\n",
    "    et = ExtraTreesClassifier(n_estimators=200, random_state=2020,learning_rate=0.01, max_depth=15,max_features=\"auto\",verbose=1)\n",
    "    et_train, et_test = stacking(et, train_x, train_y, test_x, \"et\", kf, label_split)\n",
    "    return et_train, et_test, \"et_stacker\"\n",
    "\n",
    "\n",
    "def lr_model(train_x, train_y, test_x, kf, label_split=None):\n",
    "    lr = LogisticRegression(n_jobs=-1)\n",
    "    lr_train, lr_test = stacking(lr, train_x, train_y, test_x, \"lr\", kf, label_split)\n",
    "    return lr_train, lr_test, \"lr_stacker\"\n",
    "\n",
    "def my_xgb_model(train_x, train_y, test_x, kf, label_split=None):\n",
    "    xgb_train, xgb_test = stacking(xgboost, train_x, train_y, test_x, \"xgb\", kf, label_split)\n",
    "    return xgb_train, xgb_test, \"xgb_stacker\"\n",
    "\n",
    "def my_lgb_model(train_x, train_y, test_x, kf, label_split=None):\n",
    "    lgb_train, lgb_test = stacking(lightgbm, train_x, train_y, test_x, \"lgb\", kf, label_split)\n",
    "    return lgb_train, lgb_test, \"lgb_stacker\"\n",
    "\n",
    "def my_cat_model(train_x, train_y, test_x, kf, label_split=None):\n",
    "    cat_train, cat_test = stacking(CatBoostRegressor, train_x, train_y, test_x, \"cat\", kf, label_split)\n",
    "    return cat_train, cat_test, \"cat_stacker\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacking_pred(train_x, train_y, test_x, kf, clf_list, label_split=None, clf_fin=\"lgb\", if_concat_origin=True):\n",
    "    print(datetime.datetime.now())\n",
    "    column_list = []\n",
    "    train_data_list = []\n",
    "    test_data_list = []\n",
    "    for i, clf_list in enumerate(clf_list):\n",
    "        clf_list = [clf_list]\n",
    "\n",
    "        for clf in clf_list:\n",
    "            train_data, test_data, clf_name = clf(train_x, train_y, test_x, kf, label_split)\n",
    "            train_data_list.append(train_data)\n",
    "            test_data_list.append(test_data)\n",
    "            column_list.append(\"clf_%s\"%(clf_name))\n",
    "    train = np.concatenate(train_data_list, axis=1)\n",
    "    test = np.concatenate(test_data_list, axis=1)\n",
    "    if if_concat_origin:\n",
    "        train = np.concatenate([train_x, train], axis=1)\n",
    "        test = np.concatenate([test_x, test], axis=1)\n",
    "    print(train_x.shape)\n",
    "    print(train.shape)\n",
    "    print(column_list)\n",
    "    if clf_fin == \"xgb\":\n",
    "        print('second layer model:', clf_fin)\n",
    "        clf = xgboost\n",
    "        train_matrix = clf.DMatrix(train , label=train_y)\n",
    "        valid_matrix = clf.DMatrix(train , label=train_y)\n",
    "        params = {\n",
    "            'booster': 'gbtree',\n",
    "            'objective': 'binary:logistic',\n",
    "            'eval_metric': 'auc',\n",
    "            'gamma': 1,\n",
    "            'min_child_weight': 1.5,\n",
    "            'max_depth': 5,\n",
    "            'lambda': 10,\n",
    "            'subsample': 0.7,\n",
    "            'colsample_bytree': 0.7,\n",
    "            'colsample_bylevel': 0.7,\n",
    "            'eta': 0.04,\n",
    "            'tree_method': 'exact',\n",
    "            'seed': 2019,\n",
    "            \"silent\": True,\n",
    "        }\n",
    "        watchlist = [(train_matrix, 'train'),(valid_matrix, 'eval')]\n",
    "        model = clf.train(params, train_matrix, num_boost_round=5000, evals=watchlist, verbose_eval=200, early_stopping_rounds=200)\n",
    "\n",
    "        pre = model.predict(clf.DMatrix(test), ntree_limit=model.best_ntree_limit).reshape(-1,1)\n",
    "        return pre, train, test\n",
    "    if clf_fin == 'lgb':\n",
    "        print('second layer model:', clf_fin)\n",
    "        clf = lightgbm\n",
    "        train_matrix = clf.Dataset(train, label=train_y)\n",
    "        valid_matrix = clf.Dataset(train, label=train_y)\n",
    "        params = {\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': 'binary',\n",
    "            'metric': 'auc',\n",
    "            'min_child_weight': 5,\n",
    "            'num_leaves': 2 ** 5,\n",
    "            'lambda_l2': 10,\n",
    "            'feature_fraction': 0.8,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'bagging_freq': 4,\n",
    "            'learning_rate': 0.1,\n",
    "            'seed': 2018,\n",
    "            'silent': True,\n",
    "            'verbose': -1,\n",
    "        }\n",
    "        model = clf.train(params, train_matrix, 5000, valid_sets=[train_matrix, valid_matrix], verbose_eval=200,early_stopping_rounds=200)\n",
    "        pre = model.predict(test, num_iteration=model.best_iteration).reshape(-1,1)\n",
    "        return pre, train, test\n",
    "    if clf_fin == 'lr':\n",
    "        print('second layer model:', clf_fin)\n",
    "        clf = LogisticRegression(n_jobs=-1)\n",
    "        clf.fit(train, train_y)\n",
    "        pre = clf.predict_proba(test)\n",
    "        return pre, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "folds = 5\n",
    "seed = 2020\n",
    "kf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=0)\n",
    "\n",
    "clf_list = [my_lgb_model, my_xgb_model, my_cat_model ]\n",
    "#clf_list = [rf_model  ]\n",
    "\n",
    "pre, train, test = stacking_pred(train_data, target, test_data, kf, clf_list, label_split=None, clf_fin='lr', if_concat_origin=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数调整\n",
    "\n",
    "参考文章：https://zhuanlan.zhihu.com/p/76206257\n",
    "\n",
    "以lightgbm为例，使用网格搜索贪心参数，初始时选择较大学习率，加速收敛，其他参数调整后使用较小学习率提升精度：\n",
    "\n",
    "1. 调整max_depth 和 num_leaves，这两个参数基本可以确定树的大小及复杂度，可以同时调整，参考代码如下，其余参数的代码类似：\n",
    "2. 调整min_data_in_leaf 和 min_sum_hessian_in_leaf，防止树过拟合；\n",
    "3. 调整feature_fraction,通过随机选择一定比列的特征去模型中，防止过拟合;\n",
    "4. 调整bagging_fraction和bagging_freq,bagging_fraction相当于subsample样本采样，可以使bagging更快的运行，同时也可以降拟合。bagging_freq默认0，表示bagging的频率，0意味着没有使用bagging，k意味着每k轮迭代进行一次bagging；\n",
    "5. 调整lambda_l1(reg_alpha)和lambda_l2(reg_lambda),通过L1正则化和L2正则化降低过拟合；\n",
    "6. 调整cat_smooth，cat_smooth为设置每个类别拥有最小的个数，主要用于去噪；\n",
    "7. 调整学习率。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV  # Perforing grid search\n",
    "parameters = {\n",
    "    'max_depth': range(3,10,2),\n",
    "    'num_leaves': range(10, 80, 10),\n",
    "}\n",
    "\n",
    "gbm = lgb.LGBMClassifier(\n",
    "                        objective = 'binary',\n",
    "                        is_unbalance = True,\n",
    "                        metric = 'auc',\n",
    "\n",
    "                        max_depth = 9,\n",
    "                        num_leaves = 75,\n",
    "\n",
    "                        learning_rate = 0.1,\n",
    "\n",
    "                        min_child_samples = 40,\n",
    "                        min_child_weight = 1,\n",
    "\n",
    "                        colsample_bytree = 0.7,\n",
    "\n",
    "                        subsample = 0.9,\n",
    "                        subsample_freq = 4,\n",
    "\n",
    "                        reg_alpha = 0.4,\n",
    "                        reg_lambda = 35,\n",
    "\n",
    "                        cat_smooth = 0,\n",
    "                        )\n",
    "\n",
    "gsearch = GridSearchCV(gbm, param_grid=parameters, scoring='roc_auc', cv=3)\n",
    "gsearch.fit(train_data, target)\n",
    "print('参数的最佳取值:{0}'.format(gsearch.best_params_))\n",
    "print('最佳模型得分:{0}'.format(gsearch.best_score_))\n",
    "print(gsearch.cv_results_['mean_test_score'])\n",
    "print(gsearch.cv_results_['params'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python36]",
   "language": "python",
   "name": "conda-env-python36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
