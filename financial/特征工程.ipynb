{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征工程\n",
    "特征工程是从row data中提取特征的过程，这些特征应从多种角度描述数据，使得基于这些特征训练的模型具有优秀的性能，特征工程流程包括特征处理、特征选择、特征变换、特征合成等。\n",
    "\n",
    "### 特征处理\n",
    "1. 数据清洗\n",
    "\n",
    "对重复特征、无用特征进行删除："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "import pandas as pd\n",
    "train_data = pd.read_csv(\"./train.csv\", encoding='utf-8')\n",
    "test_data = pd.read_csv(\"./testA.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重复特征\n",
    "train_data = train_data.drop(['n2.1'],axis=1)\n",
    "test_data = test_data.drop(['n2.1', 'n2.2', 'n2.3'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找出可能存在的方差为0的无用，删除\n",
    "train_one_value_fea = [col for col in train_data.columns if train_data[col].nunique() <= 1]\n",
    "test_one_value_fea = [col for col in test_data.columns if test_data[col].nunique() <= 1]\n",
    "print(train_one_value_fea, test_one_value_fea)\n",
    "\n",
    "train_data = train_data.drop(['policyCode'],axis=1)\n",
    "test_data = test_data.drop(['policyCode'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 内存压缩\n",
    "\n",
    "对数据值大小修改数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 内存压缩\n",
    "def memory_compress(df, verbose=True):\n",
    "    start_mem = df.memory_usage().sum()/1024**2\n",
    "    numerics = ['int16','int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            col_min = df[col].min()\n",
    "            col_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if col_min>np.iinfo(np.int8).min and col_max<np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif col_min>np.iinfo(np.int16).min and col_max<np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif col_min>np.iinfo(np.int32).min and col_max<np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif col_min>np.iinfo(np.int64).min and col_max<np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if col_min>np.finfo(np.float16).min and col_max<np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif col_min>np.finfo(np.float32).min and col_max<np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum()/1024**2\n",
    "    print('start_mem: {:.2f} MB'.format(start_mem))\n",
    "    print('end_mem: {:.2f} MB'.format(end_mem))\n",
    "    print('compress ratio: {:.2%} MB'.format((start_mem-end_mem)/start_mem))\n",
    "    return df\n",
    "train_data = memory_compress(train_data)\n",
    "test_data = memory_compress(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([train_data, test_data], axis=0, ignore_index=True)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 非数值类型特征处理\n",
    "\n",
    "对object字段进行处理 ['grade', 'subGrade', 'employmentLength', 'issueDate', 'earliesCreditLine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA中发现grade、subGrade与违约存在着明显的序号关系\n",
    "\n",
    "# 贷款等级\n",
    "data['grade'] = data['grade'].map({'A':1,'B':7,'C':13,'D':19,'E':25,'F':31,'G':37})\n",
    "\n",
    "# 贷款子等级\n",
    "data['subGrade'] = data['subGrade'].map({'A1':2,'A2':3,'A3':4,'A4':5,'A5':6,\n",
    "                                        'B1':8,'B2':9,'B3':10,'B4':11,'B5':12,\n",
    "                                        'C1':14,'C2':15,'C3':16,'C4':17,'C5':18,\n",
    "                                        'D1':20,'D2':21,'D3':22,'D4':23,'D5':24,\n",
    "                                        'E1':26,'E2':27,'E3':28,'E4':29,'E5':30,\n",
    "                                        'F1':32,'F2':33,'F3':34,'F4':35,'F5':36,\n",
    "                                        'G1':38,'G2':39,'G3':40,'G4':41,'G5':42,})\n",
    "# 就业年限\n",
    "def employmentLength_to_int(s):\n",
    "    if pd.isnull(s):\n",
    "        return s\n",
    "    else:\n",
    "        return np.int8(s.split()[0])\n",
    "data['employmentLength'].replace(to_replace='10+ years', value='10 years', inplace=True)\n",
    "data['employmentLength'].replace('< 1 year', '0 years', inplace=True)\n",
    "data['employmentLength'] = data['employmentLength'].apply(employmentLength_to_int)\n",
    "data['employmentLength'] = data['employmentLength'].fillna(data['employmentLength'].median())\n",
    "data['employmentLength'].value_counts(dropna= False).sort_index()\n",
    "\n",
    "# 贷款发放的月份\n",
    "#train_data['issueDate'].apply(lambda x:str(x[-2:])).value_counts()\n",
    "start_date = datetime.datetime.strptime('2007-06-01', '%Y-%m-%d')\n",
    "data['issueDate'] = pd.to_datetime(data['issueDate'], format='%Y-%m-%d')\n",
    "data['issueDate'] = data['issueDate'].apply(lambda x: x-start_date).dt.days\n",
    "\n",
    "# earliesCreditLine  借款人最早报告的信用额度开立的月份\n",
    "start_date = pd.to_datetime(data['earliesCreditLine'], format='%b-%Y').min()\n",
    "data['earliesCreditLine'] = pd.to_datetime(data['earliesCreditLine'], format='%b-%Y')\n",
    "data['earliesCreditLine'] = data['earliesCreditLine'].apply(lambda x: x-start_date).dt.days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 特征选择\n",
    "\n",
    "删除EDA中相关性系数大于0.9的变量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redundancy_fea = ['id','installment','ficoRangeHigh','n10','n9','grade', 'subGrade']\n",
    "#redundancy_fea = ['ficoRangeHigh']\n",
    "#redundancy_fea = ['id','ficoRangeHigh']\n",
    "\n",
    "train_data = train_data.drop(redundancy_fea, axis=1)\n",
    "test_data = test_data.drop(redundancy_fea, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.特征转换\n",
    "\n",
    "- 归一化、Box-Cox转换，对非决策树类模型有效\n",
    "- 高维稀疏类别特征使用Label Encode转换\n",
    "- 低维类别变量使用One-hot编码\n",
    "- 数值跨域大且稀疏特征使用分箱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_01(col):\n",
    "    if col not in ['term', 'grade', 'subGrade', 'homeOwnership', 'verificationStatus','purpose','initialListStatus']:\n",
    "        return (col-col.min())/(col.max()-col.min())\n",
    "data = data.apply(scale_01, axis=0)\n",
    "\n",
    "def box_cox(data):\n",
    "    for col in data.columns:\n",
    "        if col not in ['term', 'grade', 'subGrade', 'ficoRangeLow','homeOwnership', 'verificationStatus','purpose','initialListStatus']:\n",
    "            print(col)\n",
    "            _, opt_lambda = stats.boxcox(np.array(data[col].dropna())+2)\n",
    "            data[col] = stats.boxcox(data[col]+2, lmbda=opt_lambda)\n",
    "    return data\n",
    "data = box_cox(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 高维稀疏类别特征\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "for col in ['employmentTitle', 'postCode', 'title','regionCode']:\n",
    "    #data[col+'_cnts'] = data.groupby([col])['id'].transform('count')\n",
    "    #data[col+'_rank'] = data.groupby([col])['id'].rank(ascending=False).astype(np.int32)\n",
    "    data[col+'_le'] = le.fit_transform(data[col])\n",
    "    data.pop(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 低维类别变量\n",
    "data = pd.get_dummies(data, columns=['purpose','initialListStatus','applicationType'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "def binning(x, y, nan):\n",
    "    boundary = []\n",
    "    x = x.fillna(nan).values\n",
    "    y = y.values\n",
    "    \n",
    "    clf = DecisionTreeClassifier(criterion='entropy',\n",
    "                                 max_leaf_nodes=6,\n",
    "                                 min_samples_leaf=0.05)\n",
    "\n",
    "    clf.fit(x.reshape(-1, 1), y)\n",
    "    \n",
    "    n_nodes = clf.tree_.node_count\n",
    "    children_left = clf.tree_.children_left\n",
    "    children_right = clf.tree_.children_right\n",
    "    threshold = clf.tree_.threshold\n",
    "    \n",
    "    for i in range(n_nodes):\n",
    "        if children_left[i] != children_right[i]:\n",
    "            boundary.append(threshold[i])\n",
    "    boundary.sort()\n",
    "    min_x = x.min()\n",
    "    max_x = x.max() + 0.1\n",
    "    boundary = [min_x] + boundary + [max_x]\n",
    "    return boundary\n",
    "\n",
    "# 特征分桶\n",
    "bin_list = ['employmentLength', 'issueDate', 'earliesCreditLine', 'delinquency_2years', 'pubRec', 'pubRecBankruptcies']\n",
    "offset = 800000\n",
    "for col in bin_list:\n",
    "    boundary = binning(x=data[col][:offset], y=data['isDefault'][:offset], nan=data[col][:offset].median())\n",
    "    #data[col] = pd.cut(x=data[col], bins=boundary, right=False, labels=False)\n",
    "    data[col+'_bin'] = pd.cut(x=data[col], bins=boundary, right=False, labels=False)\n",
    "    print('col: {}   boundary{}'.format(col, boundary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 特征合成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一些特征组合\n",
    "data['totalLoanAmnt'] = data['loanAmnt'].mul(data['interestRate'].apply(lambda x:1.0+0.01*x))\n",
    "data['loanAmntYear'] = data['totalLoanAmnt']/data['term']\n",
    "data['freeWealthYear'] = data['annualIncome']-data['loanAmntYear']\n",
    "\n",
    "n_pos_list = ['n0', 'n1', 'n2', 'n4', 'n6', 'n7', 'n13', 'n14']\n",
    "n_neg_list = ['n5','n8']\n",
    "\n",
    "data['n_pos']=data[n_pos_list].apply(lambda x:x.sum(),axis=1)\n",
    "data['n_neg']=data[n_neg_list].apply(lambda x:x.sum(),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. 不平衡数据集处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "smote_tomek = SMOTETomek(random_state=0)\n",
    "train_data, target = smote_tomek.fit_resample(train_data, target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python36]",
   "language": "python",
   "name": "conda-env-python36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
